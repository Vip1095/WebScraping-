{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anacon\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 12 samples\n",
      "Epoch 1/50\n",
      " - 0s - loss: 0.8464 - acc: 0.4200 - val_loss: 0.7342 - val_acc: 0.5000\n",
      "Epoch 2/50\n",
      " - 0s - loss: 0.7833 - acc: 0.3700 - val_loss: 0.6842 - val_acc: 0.5833\n",
      "Epoch 3/50\n",
      " - 0s - loss: 0.7343 - acc: 0.3600 - val_loss: 0.6464 - val_acc: 0.6667\n",
      "Epoch 4/50\n",
      " - 0s - loss: 0.7016 - acc: 0.5100 - val_loss: 0.6256 - val_acc: 0.7500\n",
      "Epoch 5/50\n",
      " - 0s - loss: 0.6840 - acc: 0.5700 - val_loss: 0.6115 - val_acc: 0.7500\n",
      "Epoch 6/50\n",
      " - 0s - loss: 0.6677 - acc: 0.6000 - val_loss: 0.6026 - val_acc: 0.8333\n",
      "Epoch 7/50\n",
      " - 0s - loss: 0.6541 - acc: 0.6300 - val_loss: 0.5952 - val_acc: 0.7500\n",
      "Epoch 8/50\n",
      " - 0s - loss: 0.6416 - acc: 0.6700 - val_loss: 0.5844 - val_acc: 0.8333\n",
      "Epoch 9/50\n",
      " - 0s - loss: 0.6279 - acc: 0.7000 - val_loss: 0.5760 - val_acc: 0.8333\n",
      "Epoch 10/50\n",
      " - 0s - loss: 0.6152 - acc: 0.7200 - val_loss: 0.5647 - val_acc: 0.8333\n",
      "Epoch 11/50\n",
      " - 0s - loss: 0.6040 - acc: 0.7300 - val_loss: 0.5574 - val_acc: 0.8333\n",
      "Epoch 12/50\n",
      " - 0s - loss: 0.5915 - acc: 0.7200 - val_loss: 0.5513 - val_acc: 0.7500\n",
      "Epoch 13/50\n",
      " - 0s - loss: 0.5803 - acc: 0.7200 - val_loss: 0.5420 - val_acc: 0.7500\n",
      "Epoch 14/50\n",
      " - 0s - loss: 0.5647 - acc: 0.7500 - val_loss: 0.5284 - val_acc: 0.7500\n",
      "Epoch 15/50\n",
      " - 0s - loss: 0.5512 - acc: 0.7900 - val_loss: 0.5180 - val_acc: 0.7500\n",
      "Epoch 16/50\n",
      " - 0s - loss: 0.5392 - acc: 0.7900 - val_loss: 0.5108 - val_acc: 0.7500\n",
      "Epoch 17/50\n",
      " - 0s - loss: 0.5291 - acc: 0.8100 - val_loss: 0.5000 - val_acc: 0.7500\n",
      "Epoch 18/50\n",
      " - 0s - loss: 0.5181 - acc: 0.8100 - val_loss: 0.4950 - val_acc: 0.7500\n",
      "Epoch 19/50\n",
      " - 0s - loss: 0.5091 - acc: 0.8100 - val_loss: 0.4911 - val_acc: 0.7500\n",
      "Epoch 20/50\n",
      " - 0s - loss: 0.5010 - acc: 0.8100 - val_loss: 0.4869 - val_acc: 0.7500\n",
      "Epoch 21/50\n",
      " - 0s - loss: 0.4938 - acc: 0.8300 - val_loss: 0.4828 - val_acc: 0.7500\n",
      "Epoch 22/50\n",
      " - 0s - loss: 0.4855 - acc: 0.8200 - val_loss: 0.4798 - val_acc: 0.7500\n",
      "Epoch 23/50\n",
      " - 0s - loss: 0.4773 - acc: 0.8300 - val_loss: 0.4744 - val_acc: 0.7500\n",
      "Epoch 24/50\n",
      " - 0s - loss: 0.4700 - acc: 0.8300 - val_loss: 0.4691 - val_acc: 0.7500\n",
      "Epoch 25/50\n",
      " - 0s - loss: 0.4641 - acc: 0.8300 - val_loss: 0.4624 - val_acc: 0.7500\n",
      "Epoch 26/50\n",
      " - 0s - loss: 0.4566 - acc: 0.8300 - val_loss: 0.4626 - val_acc: 0.7500\n",
      "Epoch 27/50\n",
      " - 0s - loss: 0.4516 - acc: 0.8300 - val_loss: 0.4585 - val_acc: 0.7500\n",
      "Epoch 28/50\n",
      " - 0s - loss: 0.4447 - acc: 0.8300 - val_loss: 0.4593 - val_acc: 0.7500\n",
      "Epoch 29/50\n",
      " - 0s - loss: 0.4390 - acc: 0.8400 - val_loss: 0.4565 - val_acc: 0.7500\n",
      "Epoch 30/50\n",
      " - 0s - loss: 0.4360 - acc: 0.8400 - val_loss: 0.4539 - val_acc: 0.7500\n",
      "Epoch 31/50\n",
      " - 0s - loss: 0.4294 - acc: 0.8400 - val_loss: 0.4471 - val_acc: 0.7500\n",
      "Epoch 32/50\n",
      " - 0s - loss: 0.4234 - acc: 0.8400 - val_loss: 0.4452 - val_acc: 0.7500\n",
      "Epoch 33/50\n",
      " - 0s - loss: 0.4192 - acc: 0.8400 - val_loss: 0.4440 - val_acc: 0.7500\n",
      "Epoch 34/50\n",
      " - 0s - loss: 0.4144 - acc: 0.8400 - val_loss: 0.4410 - val_acc: 0.8333\n",
      "Epoch 35/50\n",
      " - 0s - loss: 0.4105 - acc: 0.8400 - val_loss: 0.4398 - val_acc: 0.8333\n",
      "Epoch 36/50\n",
      " - 0s - loss: 0.4066 - acc: 0.8400 - val_loss: 0.4350 - val_acc: 0.8333\n",
      "Epoch 37/50\n",
      " - 0s - loss: 0.4027 - acc: 0.8400 - val_loss: 0.4333 - val_acc: 0.8333\n",
      "Epoch 38/50\n",
      " - 0s - loss: 0.3995 - acc: 0.8400 - val_loss: 0.4338 - val_acc: 0.8333\n",
      "Epoch 39/50\n",
      " - 0s - loss: 0.3965 - acc: 0.8300 - val_loss: 0.4285 - val_acc: 0.8333\n",
      "Epoch 40/50\n",
      " - 0s - loss: 0.3932 - acc: 0.8300 - val_loss: 0.4277 - val_acc: 0.8333\n",
      "Epoch 41/50\n",
      " - 0s - loss: 0.3902 - acc: 0.8300 - val_loss: 0.4252 - val_acc: 0.8333\n",
      "Epoch 42/50\n",
      " - 0s - loss: 0.3878 - acc: 0.8300 - val_loss: 0.4228 - val_acc: 0.8333\n",
      "Epoch 43/50\n",
      " - 0s - loss: 0.3849 - acc: 0.8300 - val_loss: 0.4213 - val_acc: 0.8333\n",
      "Epoch 44/50\n",
      " - 0s - loss: 0.3825 - acc: 0.8300 - val_loss: 0.4206 - val_acc: 0.8333\n",
      "Epoch 45/50\n",
      " - 0s - loss: 0.3813 - acc: 0.8400 - val_loss: 0.4236 - val_acc: 0.8333\n",
      "Epoch 46/50\n",
      " - 0s - loss: 0.3771 - acc: 0.8300 - val_loss: 0.4166 - val_acc: 0.8333\n",
      "Epoch 47/50\n",
      " - 0s - loss: 0.3751 - acc: 0.8400 - val_loss: 0.4136 - val_acc: 0.8333\n",
      "Epoch 48/50\n",
      " - 0s - loss: 0.3736 - acc: 0.8400 - val_loss: 0.4111 - val_acc: 0.8333\n",
      "Epoch 49/50\n",
      " - 0s - loss: 0.3707 - acc: 0.8400 - val_loss: 0.4143 - val_acc: 0.8333\n",
      "Epoch 50/50\n",
      " - 0s - loss: 0.3687 - acc: 0.8400 - val_loss: 0.4137 - val_acc: 0.8333\n",
      "Train accuracy:  [0.3667430281639099, 0.8400000035762787]\n",
      "Test accuracy:  [0.4137359857559204, 0.8333333134651184]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e3b99af28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 14)                196       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 30        \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#!usr/bin/python\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation , MaxPool2D , Conv2D , Flatten\n",
    "from keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def readFeaturesFile():\n",
    "\tnames = ['Feature1', 'Feature2', 'Feature3', 'Feature4','Feature5','Feature6','Feature7','Feature8','Feature9',\n",
    "'Feature10','Feature11','Feature12','Feature13','Gender']\n",
    "\tdata = pd.read_csv(\"E:\\Vipul\\ML\\MLP\\Specch\\mfcc_featuresLR.txt\",names=names )\n",
    "\t#the outcome is a list of lists containing the samples with the following format\n",
    "\t#[charachteristic,feature1,feature2.......,feature13]\n",
    "\t#characheristic based on what we want for classification , can be (male , female) , also can be (normal-female,edema-female)\n",
    "\t#in general characheristic is the target value .\n",
    "\treturn data\n",
    "\n",
    "def preparingData(data):\n",
    "\t# Split-out validation dataset\n",
    "\tarray = data.values\n",
    "\t#input\n",
    "\tX = array[:,0:13]\n",
    "\t#target \n",
    "\tY = array[:,13]\n",
    "\t\n",
    "\t#determine the test and the training size\n",
    "\tx_train, x_test, y_train, y_test = train_test_split(X,Y, test_size=0.10)\n",
    "\t\n",
    "\n",
    "\t#x_train = \n",
    "\t#Encode the labels\n",
    "\t#reconstruct the data as a vector with a sequence of 1s and 0s\n",
    "\ty_train = keras.utils.to_categorical(y_train, num_classes = 2)\n",
    "\ty_test = keras.utils.to_categorical(y_test, num_classes = 2)\n",
    "\t'''print y_train.shape\n",
    "\tprint x_train.shape\n",
    "\tprint(y_train[0], np.argmax(y_train[0]))\n",
    "\tprint(y_train[1], np.argmax(y_train[1]))\n",
    "\tprint(y_train[2], np.argmax(y_train[2]))\n",
    "\tprint(y_train[3], np.argmax(y_train[3]))'''\n",
    "\treturn x_train , x_test , y_train , y_test \n",
    "\n",
    "\n",
    "def returnData(data):\n",
    "\t# Split-out validation dataset\n",
    "\tarray = data.values\n",
    "\t#input\n",
    "\tX = array[:,0:13]\n",
    "\t#target \n",
    "\tY = array[:,13]\n",
    "\t\n",
    "\t#determine the test and the training size\n",
    "\t\n",
    "\treturn X,Y\n",
    "\n",
    "\n",
    "\n",
    "#Multilayer Perceptron\n",
    "def testing_NN(data):\n",
    "\tX,Y = returnData(data)\n",
    "\t\n",
    "\t#determine the validation\n",
    "\tkfold = StratifiedKFold(n_splits=10,shuffle=True)\n",
    "\t#keep the results\n",
    "\tcvscores = []\n",
    "\tfor train,test in kfold.split(X,Y):\n",
    "\t\t#Define a siple Multilayer Perceptron\n",
    "\t\tmodel = Sequential()\n",
    "\n",
    "\t\t#our classification is binary\n",
    "\n",
    "\t\t#as a first step we have to define the input dimensionality\n",
    "\n",
    "\n",
    "\t\tmodel.add(Dense(14,activation='relu',input_dim=13))\n",
    "\n",
    "\n",
    "\t\t#model.add(Dense(14,activation='relu',input_dim=13))\n",
    "\t\tmodel.add(Dense(8, activation='relu'))\n",
    "\n",
    "\t\t#add another hidden layer\n",
    "\t\t#model.add(Dense(16,activation='relu'))\n",
    "\t\t#the last step , add an output layer (number of neurons = number of classes)\n",
    "\t\tmodel.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "\t\t#select the optimizer\n",
    "\t\t#adam = Adam(lr=0.0001)\n",
    "\t\tadam = Adam(lr=0.001)\n",
    "\t\t#learning rate is between 0.0001 and 0.001 , but it is objective to define it\n",
    "\t\t#because we need out model not to learn to fast and maybe we have overfitting but also \n",
    "\t\t#not to slow and take to much time . We can check this with the learning rate curve\n",
    "\t\n",
    "\t\t#we select the loss function and metrics that should be monitored\n",
    "\t\t#and then we compile our model\n",
    "\t\tmodel.compile(loss='binary_crossentropy',optimizer=adam,metrics=['accuracy'])\n",
    "\n",
    "\t\t#now we train our model\n",
    "\t\tmodel.fit(X[train],Y[train],epochs=50,batch_size=75,verbose=0)\n",
    "\n",
    "\t\t\t# evaluate the model\n",
    "\t\tscores = model.evaluate(X[test], Y[test], verbose=0)\n",
    "\t\tprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\t\tcvscores.append(scores[1] * 100)\n",
    "\n",
    "\n",
    "\tprint(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "\n",
    "\n",
    "\t'''x_train , x_test , y_train , y_test = preparingData(data)\n",
    "\t\n",
    "\t#validation data =  test data\n",
    "\t#only for the plot\n",
    "\tresults = model.fit(x_train,y_train,epochs=50,batch_size=75,verbose=2,validation_data=(x_test,y_test))\n",
    "\n",
    "\tplt.figure(1)\n",
    "\tplt.plot(results.history['loss'])\n",
    "\tplt.plot(results.history['val_loss'])\n",
    "\tplt.legend(['train loss', 'test loss'])\n",
    "\tplt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\t#now we can evaluate our model\n",
    "\tprint '\\n'\n",
    "\tprint 'Train accuracy: ' , model.evaluate(x_train,y_train,batch_size=25)\n",
    "\tprint 'Test accuracy: ',model.evaluate(x_test,y_test,batch_size=25)\n",
    "\n",
    "\t#visualize the actual output of the network\n",
    "\toutput = model.predict(x_train)\n",
    "\tprint '\\n'\n",
    "\tprint 'Actual output: ',output[0],np.argmax(output[0])\n",
    "\n",
    "\t#we can also check our model behaviour in depth\n",
    "\tprint'\\n'\n",
    "\t#print the first ten predictions\n",
    "\tfor x in range(10):\n",
    "\t\tprint 'Prediction: ',np.argsort(output[x])[::-1],'True target: ',np.argmax(y_train[x])'''\n",
    "\n",
    "#Multilayer Perceptron\n",
    "def simpleNN(data):\n",
    "\tx_train , x_test , y_train , y_test = preparingData(data)\n",
    "\n",
    "\t#because as we can see from the previous function simpleNN the\n",
    "\t#test loss is going bigger which means that we have overfitting problem\n",
    "\t#here we are going to try to overcome this obstacle\n",
    "\n",
    "\tmodel = Sequential()\n",
    "\n",
    "\t#The input layer:\n",
    "\t'''With respect to the number of neurons comprising this layer, this parameter is completely and uniquely determined \n",
    "\tonce you know the shape of your training data. Specifically, the number of neurons comprising that layer is equal to the number \n",
    "\tof features (columns) in your data. Some NN configurations add one additional node for a bias term.'''\n",
    "\n",
    "\tmodel.add(Dense(14,activation='relu',input_dim=13,kernel_initializer='random_uniform'))\n",
    "\t\n",
    "\t#The output layer\n",
    "\t'''If the NN is a classifier, then it also has a single node unless\n",
    "\t softmax is used in which case the output layer has one node per \n",
    "\tclass label in your model.'''\n",
    "\t\n",
    "\tmodel.add(Dense(2,activation='softmax'))\n",
    "\n",
    "\t#binary_crossentropy because we have a binary classification model\n",
    "\t#Because it is not guaranteed that we are going to find the global optimum\n",
    "\t#because we can be trapped in a local minima and the algorithm may think that\n",
    "\t#you reach global minima. To avoid this situation, we use a momentum term in the \n",
    "\t#objective function, which is a value 0 < momentum < 1 , that increases the size of the steps\n",
    "\t#taken towards the minimum by trying to jump from a local minima.\n",
    "\n",
    "\n",
    "\t#If the momentum term is large then the learning rate should be kept smaller.\n",
    "\t#A large value of momentum means that the convergence will happen fast,but if\n",
    "\t#both are kept at large values , then we might skip the minimum with a huge step.\n",
    "\t#A small value of momentum cannot reliably avoid local minima, and also slow down\n",
    "\t#the training system. We are trying to find the right value of momentum through cross-validation.\t\n",
    "\tmodel.compile(loss='binary_crossentropy',optimizer=SGD(lr=0.001,momentum=0.6),metrics=['accuracy'])\n",
    "\n",
    "\t#In simple terms , learning rate is how quickly a network abandons old beliefs for new ones.\n",
    "\t#Which means that with a higher LR the network changes its mind more quickly , in pur case this means\n",
    "\t#how quickly our model update the parameters (weights,bias).\n",
    "\n",
    "\t#verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "\n",
    "\tresults = model.fit(x_train,y_train,epochs=50,batch_size=50,verbose=2,validation_data=(x_test,y_test))\n",
    "\n",
    "\tprint ('Train accuracy: ' , model.evaluate(x_train,y_train,batch_size=50,verbose=2))\n",
    "\tprint ('Test accuracy: ',model.evaluate(x_test,y_test,batch_size=50,verbose=2))\n",
    "\n",
    "\n",
    "\t\n",
    "\t#visualize\n",
    "\tplt.figure(1)\n",
    "\tplt.plot(results.history['loss'])\n",
    "\tplt.plot(results.history['val_loss'])\n",
    "\tplt.legend(['train loss', 'test loss'])\n",
    "\tplt.show()\n",
    "\n",
    "\tprint (model.summary())\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\tdata = readFeaturesFile()\n",
    "\tsimpleNN(data)\n",
    "\t\t\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b955de34cbb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#visualize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x26e3cf2e470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\t#visualize\n",
    "\tplt.figure(1)\n",
    "\tplt.plot(results.history['loss'])\n",
    "\tplt.plot(results.history['val_loss'])\n",
    "\tplt.legend(['train loss', 'test loss'])\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
